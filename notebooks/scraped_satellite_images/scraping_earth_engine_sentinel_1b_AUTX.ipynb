{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook scrapes satellite images for each leak repair. For each location it gets a NxM rectangle around the leak before and after it was repaired. Then it collated all the data into h5 files and all the metadata into json files.\n",
    "\n",
    "It takes days to run because of rate limiting on the google earth api. Because of limited satelite coverage you might find matches for only 10% of the leaks.\n",
    "\n",
    "## Modifying\n",
    "\n",
    "- make sure google earth is setup\n",
    "- load leaks, so they pass the asserts\n",
    "- change params\n",
    "- run rest of cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-15T09:09:12.184448Z",
     "start_time": "2017-03-15T17:09:11.455946+08:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%.4f'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from path import Path\n",
    "import arrow\n",
    "import json\n",
    "import pytz\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import re, os, collections, itertools, uuid, logging\n",
    "import tempfile\n",
    "import tables\n",
    "\n",
    "import zipfile\n",
    "import urllib\n",
    "\n",
    "import ee\n",
    "import pyproj\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (15, 5) # bigger plots\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "%precision 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-15T09:09:12.187679Z",
     "start_time": "2017-03-15T17:09:12.185820+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-15T09:09:15.141885Z",
     "start_time": "2017-03-15T17:09:12.189249+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "helper_dir = str(Path('..').abspath())\n",
    "if helper_dir not in os.sys.path:\n",
    "    os.sys.path.append(helper_dir)\n",
    "    \n",
    "from leak_helpers.earth_engine import display_ee, get_boundary, tifs2np, bands_s2, download_image, bands_s2, bands_s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-15T09:09:15.161041Z",
     "start_time": "2017-03-15T17:09:15.143507+08:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Path('/tmp/testing_earth_engine-s1-AUTX_v3-v5sl3376-20170315-09-09-15'),\n",
       " Path('../data/20170314-05-26-52_testing_earth_engine-s1-AUTX_v3'),\n",
       " Path('../data/20170314-05-26-52_testing_earth_engine-s1-AUTX_v3/ee_S1_AUTX-leaks_cache_v3'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "crs_grid = 3857\n",
    "notebook_name='testing_earth_engine-s1-AUTX_v3'\n",
    "ts=arrow.utcnow().format('YYYYMMDD-HH-mm-ss')\n",
    "data_dir = Path('../../data/')\n",
    "bands = bands_s1\n",
    "\n",
    "# since the lowest res band is 60m and I want to capture neighbours I should get 6+ pixels\n",
    "pixel_length = 25.0\n",
    "resolution_min = 10.0 # m\n",
    "time_bin_delta = 60*60*24*28 # how long before a leak to look (in seconds)\n",
    "# TODO get closest but let me filter for time\n",
    "\n",
    "# init\n",
    "temp_dir = Path(tempfile.mkdtemp(prefix=notebook_name+'-', suffix='-'+ts))\n",
    "# output_dir = data_dir.joinpath('{ts:}_{notebook_name:}'.format(ts=ts,notebook_name=notebook_name))\n",
    "output_dir = Path('../../data/scraped_satellite_images/20170314-05-26-52_testing_earth_engine-s1-AUTX_v3')\n",
    "cache_dir = output_dir.joinpath('ee_S1_AUTX-leaks_cache_v3')\n",
    "\n",
    "output_dir.makedirs_p()\n",
    "temp_dir.makedirs_p()\n",
    "cache_dir.makedirs_p()\n",
    "\n",
    "logger = logging.getLogger(notebook_name)\n",
    "logger.setLevel(logging.WARN)\n",
    "\n",
    "crs_grid_proj = pyproj.Proj('+init=epsg:%s'%crs_grid)\n",
    "\n",
    "temp_dir, output_dir, cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-15T09:09:15.172715Z",
     "start_time": "2017-03-15T17:09:15.162936+08:00"
    }
   },
   "outputs": [],
   "source": [
    "# open hdf5 data files\n",
    "# X_file = output_dir.joinpath('X_train.hdf5')\n",
    "# y_file = output_dir.joinpath('y_train.hdf5')\n",
    "metadata_file = output_dir.joinpath('metadata.json')\n",
    "\n",
    "# write metadata to json\n",
    "metadata = dict(\n",
    "    pixel_length=pixel_length,\n",
    "    resolution_min=resolution_min,\n",
    "    bands=bands,\n",
    "    ts=ts,\n",
    "    notebook_name=notebook_name,\n",
    "    crs_grid=crs_grid,\n",
    "    cache_dir=str(cache_dir),\n",
    "    temp_dir=str(temp_dir),\n",
    "    output_dir=str(output_dir),\n",
    ")\n",
    "json.dump(metadata, open(metadata_file,'w'))\n",
    "\n",
    "# write headers\n",
    "\n",
    "# with tables.open_file(X_file, 'w') as xfo:\n",
    "#     atom = tables.Atom.from_dtype(np.dtype('Float32'))\n",
    "#     # create an expandable array\n",
    "#     data_storage = xfo.create_earray(xfo.root, 'data', atom, (0,len(bands),pixel_length,pixel_length))\n",
    "                                     \n",
    "\n",
    "# # write headers for y\n",
    "# with tables.open_file(y_file, 'w') as xfo:\n",
    "#     atom = tables.Atom.from_dtype(np.dtype('int'))\n",
    "#     # create an expandable array\n",
    "#     data_storage = xfo.create_earray(xfo.root, 'data', atom, (0,))\n",
    "                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# earth engine\n",
    "\n",
    "Setup instructions here\n",
    "- first need to apply for an account and wait ~ 1day\n",
    "- https://developers.google.com/earth-engine/python_install#setting-up-authentication-credentials\n",
    "\n",
    "Refs:\n",
    "- api https://developers.google.com/earth-engine/\n",
    "- code examples https://code.earthengine.google.com/\n",
    "- sentinel1 https://developers.google.com/earth-engine/sentinel1\n",
    "    - `ee.ImageCollection('COPERNICUS/S2_GRD');`\n",
    "    - `ee.ImageCollection('COPERNICUS/S1_GRD');`\n",
    "- keras and google earth https://github.com/patrick-dd/landsat-landstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-15T09:09:17.578957Z",
     "start_time": "2017-03-15T17:09:15.173930+08:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "# test earth-engine setup\n",
    "from oauth2client import crypt # should have not error\n",
    "import ee\n",
    "ee.Initialize() # should give no errors, if so follow instructions\n",
    "\n",
    "\n",
    "# test\n",
    "image = ee.Image('srtm90_v4')\n",
    "assert image.getInfo()=={'type': 'Image', 'properties': {'system:time_start': 950227200000, 'system:asset_size': 18827626666, 'system:time_end': 951177600000}, 'bands': [{'data_type': {'type': 'PixelType', 'max': 32767, 'min': -32768, 'precision': 'int'}, 'crs': 'EPSG:4326', 'id': 'elevation', 'dimensions': [432000, 144000], 'crs_transform': [0.000833333333333, 0.0, -180.0, 0.0, -0.000833333333333, 60.0]}], 'id': 'srtm90_v4', 'version': 1463778555689000}\n",
    "print('ok')\n",
    "\n",
    "# ee.Geometry.Point([117.21079620254062, -30.94712385398404])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load leaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-15T09:09:19.886617Z",
     "start_time": "2017-03-15T17:09:17.580580+08:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4796"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load wa leaks\n",
    "leaks_ATX = gpd.read_file(data_dir.joinpath('leak_datasets/austin_leaks/derived/austin_leaks-repairs.geojson'))\n",
    "\n",
    "\n",
    "# they have to be after launch\n",
    "s3_launch_ts=pd.Timestamp('3 Oct 2014')\n",
    "leaks_ATX = leaks_ATX[pd.to_datetime(leaks_ATX.COMPDTTM)>=s3_launch_ts]\n",
    "len(leaks_ATX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-14T05:46:36.432522Z",
     "start_time": "2017-03-14T13:46:36.410901+08:00"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-15T09:09:19.905961Z",
     "start_time": "2017-03-15T17:09:19.888321+08:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>22</th>\n",
       "      <th>ADDRKEY</th>\n",
       "      <th>CITY</th>\n",
       "      <th>COMPDTTM</th>\n",
       "      <th>DESCRIPT</th>\n",
       "      <th>FullStreetName</th>\n",
       "      <th>INITDTTM</th>\n",
       "      <th>LOC</th>\n",
       "      <th>OBJECTID</th>\n",
       "      <th>PREDIR</th>\n",
       "      <th>QTYCALLS</th>\n",
       "      <th>STNAME</th>\n",
       "      <th>STNO</th>\n",
       "      <th>STSUB</th>\n",
       "      <th>SUFFIX</th>\n",
       "      <th>WONO</th>\n",
       "      <th>ZIP</th>\n",
       "      <th>geometry</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21545</th>\n",
       "      <td>489752.0</td>\n",
       "      <td>387141.0</td>\n",
       "      <td>AUSTIN</td>\n",
       "      <td>2016-05-02T23:15:00</td>\n",
       "      <td>WATER SERVICE LEAK</td>\n",
       "      <td>4701 WILD BRIAR PASS</td>\n",
       "      <td>2016-05-02T17:11:00</td>\n",
       "      <td>None</td>\n",
       "      <td>69032</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>WILD BRIAR</td>\n",
       "      <td>4701</td>\n",
       "      <td></td>\n",
       "      <td>PASS</td>\n",
       "      <td>1746780.0</td>\n",
       "      <td>78746-</td>\n",
       "      <td>POINT (-97.82243430021848 30.27710674017273)</td>\n",
       "      <td>69031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             22   ADDRKEY    CITY             COMPDTTM            DESCRIPT  \\\n",
       "21545  489752.0  387141.0  AUSTIN  2016-05-02T23:15:00  WATER SERVICE LEAK   \n",
       "\n",
       "             FullStreetName             INITDTTM   LOC  OBJECTID PREDIR  \\\n",
       "21545  4701 WILD BRIAR PASS  2016-05-02T17:11:00  None     69032          \n",
       "\n",
       "       QTYCALLS      STNAME  STNO STSUB SUFFIX       WONO         ZIP  \\\n",
       "21545         2  WILD BRIAR  4701         PASS  1746780.0  78746-       \n",
       "\n",
       "                                           geometry     id  \n",
       "21545  POINT (-97.82243430021848 30.27710674017273)  69031  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose one leak for now\n",
    "leak = leaks_ATX.sample()\n",
    "leak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-15T09:09:19.976162Z",
     "start_time": "2017-03-15T17:09:19.907207+08:00"
    }
   },
   "outputs": [],
   "source": [
    "leaks_ATX['REPO_Date']=leaks_ATX['COMPDTTM']\n",
    "leaks_ATX['leak_id']=leaks_ATX.OBJECTID.apply(lambda x:'ATX_%s'%x)\n",
    "# # some leaks say REPO_data some say COMPDTTM I'll unify them\n",
    "# for mm in m:\n",
    "#     props = mm['leak']['features'][0]['properties']\n",
    "#     if 'REPO_Date' not in props:\n",
    "#         props['REPO_Date']=props['COMPDTTM']\n",
    "#     if 'leak_id' not in props:\n",
    "#         props['leak_id']='AU_%s'%props['id']\n",
    "# leaks_ATX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-15T10:46:31.548298",
     "start_time": "2017-01-15T10:46:31.546367"
    }
   },
   "source": [
    "# Fetching sentinal-1 and sentinel 2 images\n",
    "\n",
    "For a leak repair, grab the image before and after it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note roughly 10% have results for a 1 day temporal bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-15T09:09:19.996205Z",
     "start_time": "2017-03-15T17:09:19.978195+08:00"
    }
   },
   "outputs": [],
   "source": [
    "def get_cached_ids():\n",
    "    cache_dirs = [str(f.relpath(cache_dir)).split('_')[0] for f in cache_dir.listdir()]\n",
    "    return cache_dirs\n",
    "\n",
    "def init_cache(leak_id):\n",
    "    \"\"\"We will cache downloads in folders like 'id_after'\"\"\"\n",
    "    if leak_id:\n",
    "        cache_subdir = cache_dir.joinpath(leak_id+'_after')\n",
    "        cache_subdir.makedirs_p()\n",
    "        cache_subdir = cache_dir.joinpath(leak_id+'_before')\n",
    "        cache_subdir.makedirs_p()\n",
    "    return get_cached_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each point\n",
    "- find the nearest image before the repair\n",
    "- and the soonest image after repair\n",
    "- save a part of each with metadata\n",
    "\n",
    "Later we can filter, interpolate, and read into numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-15T09:09:20.010843Z",
     "start_time": "2017-03-15T17:09:19.997631+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distance = resolution_min*(pixel_length/2.0-0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-15T03:23:25.601884Z",
     "start_time": "2017-03-15T11:22:57.913897+08:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 [((25, 25), -4167.8394), ((25, 25), 0.0), ((25, 25), -8604.7656), ((25, 25), 0.0), ((25, 25), 25723.6)]\n",
      "50 [((25, 25), -5057.6924), ((25, 25), 0.0), ((25, 25), -7674.0732), ((25, 25), 0.0), ((25, 25), 25614.271)]\n",
      "1000 [((25, 25), -6142.1606), ((25, 25), 0.0), ((25, 25), -10457.014), ((25, 25), 0.0), ((25, 25), 25792.426)]\n",
      "2000 [((25, 25), -5779.6787), ((25, 25), 0.0), ((25, 25), -8787.1338), ((25, 25), 0.0), ((25, 25), 25433.117)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# test with one image\n",
    "for i in [10,50,1000,2000]:\n",
    "    leak=leaks_ATX.iloc[[i]]\n",
    "    leak_id = str(leak.OBJECTID.values[0])\n",
    "\n",
    "    repo_date_ts = arrow.get(leak.REPO_Date.values[0]).timestamp\n",
    "    boundary = get_boundary(leak, distance=distance)\n",
    "    sentinel2_before = ee.ImageCollection('COPERNICUS/S1_GRD')\\\n",
    "        .filterBounds(boundary)\\\n",
    "        .filterDate(933828614605,1488776737937)\\\n",
    "        .sort('system:time_start', opt_ascending=False) # first will be latest\n",
    "    image = ee.Image(sentinel2_before.first()).clip(boundary)\n",
    "    image.getInfo()\n",
    "    name=leak_id+'_after'\n",
    "    path,files=download_image(\n",
    "        image, \n",
    "        scale=resolution_min, \n",
    "        crs=crs_grid, \n",
    "        name=name,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "    data = tifs2np(path,files,bands=bands_s1)\n",
    "    print(i,[(d.shape,d.sum()) for d in data])\n",
    "        assert d.shape[0]==pixel_length, 'the downloaded image is the wrong size, tweak distance'\n",
    "        assert d.shape[1]==pixel_length\n",
    "    assert np.sum(data)!==0,'should not be empty (make sure you are using the right bands)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-15T01:47:11.606656Z",
     "start_time": "2017-03-15T09:47:11.595047+08:00"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-14T23:17:38.739022Z",
     "start_time": "2017-03-15T07:17:38.735119+08:00"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-15T13:25:17.197398Z",
     "start_time": "2017-03-15T17:09:20.012351+08:00"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e33a7846cbd4c8293acc997c867eed8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cached_ids = get_cached_ids()\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.WARN)\n",
    "\n",
    "def get_image_for_leak(i, cached_ids=cached_ids):    \n",
    "    leak = leaks_ATX.iloc[[i]]\n",
    "    repo_date_ts = arrow.get(leak.REPO_Date.values[0]).timestamp\n",
    "    \n",
    "    \n",
    "    # crappy way or recording that we tried this one\n",
    "    leak_id = str(leak.OBJECTID.values[0])\n",
    "    if leak_id in cached_ids:\n",
    "        logger.info('Skipping cached download for leak id %s ',leak_id)\n",
    "        return\n",
    "    \n",
    "    boundary = get_boundary(leak.geometry, distance=distance)\n",
    "    \n",
    "    # get image day before    \n",
    "    sentinel2_before = ee.ImageCollection('COPERNICUS/S1_GRD')\\\n",
    "        .filterBounds(boundary)\\\n",
    "        .filterDate((repo_date_ts-time_bin_delta)*1000,(repo_date_ts)*1000)\\\n",
    "        .sort('system:time_start', opt_ascending=False) # first will be latest\n",
    "    \n",
    "    results = sentinel2_before.size().getInfo()\n",
    "    if results<1:\n",
    "        logger.info('Error no results for day before %s',leak_id)\n",
    "        cached_ids = init_cache(leak_id) # so we know there where no results\n",
    "        return\n",
    "        \n",
    "    # get image day after\n",
    "    sentinel2_after = ee.ImageCollection('COPERNICUS/S1_GRD')\\\n",
    "        .filterBounds(boundary)\\\n",
    "        .filterDate((repo_date_ts)*1000,(repo_date_ts+time_bin_delta*6)*1000)\\\n",
    "        .sort('system:time_start', opt_ascending=True) # first will be earliest\n",
    "        \n",
    "    results = sentinel2_after.size().getInfo()\n",
    "    if results<1:\n",
    "        logger.info('Error no results for day after, id %s',leak_id)\n",
    "        cached_ids = init_cache(leak_id) # so we know there where no results\n",
    "        return\n",
    "        \n",
    "    # download as save images    \n",
    "    logger.info('results for %s', leak_id)\n",
    "    image = ee.Image(sentinel2_before.first()).clip(boundary)\n",
    "    name=leak_id+'_before'\n",
    "    path,files=download_image(\n",
    "        image, \n",
    "        scale=resolution_min, \n",
    "        crs=crs_grid, \n",
    "        name=name,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "    # also save metadata so we can filter by date\n",
    "    with open(path.joinpath('metadata.json'), 'w') as fo:\n",
    "        metadata = dict(\n",
    "            image=image.getInfo(),\n",
    "            scale=resolution_min,\n",
    "            crs=crs_grid,\n",
    "            name=name,\n",
    "            distance=distance,\n",
    "            leak=json.loads(leak.to_json())\n",
    "        )\n",
    "        json.dump(metadata, fo)\n",
    "\n",
    "    image = ee.Image(sentinel2_after.first()).clip(boundary)\n",
    "    name=leak_id+'_after'\n",
    "    path,files=download_image(\n",
    "        image, \n",
    "        scale=resolution_min, \n",
    "        crs=crs_grid, \n",
    "        name=name,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "    with open(path.joinpath('metadata.json'), 'w') as fo:\n",
    "        metadata = dict(\n",
    "            image=image.getInfo(),\n",
    "            scale=resolution_min,\n",
    "            crs=crs_grid,\n",
    "            name=name,\n",
    "            distance=distance,\n",
    "            leak=json.loads(leak.to_json())\n",
    "        )\n",
    "        json.dump(metadata, fo)\n",
    "        \n",
    "for i in tqdm(range(len(leaks_ATX))):\n",
    "    try:\n",
    "        get_image_for_leak(i)\n",
    "    except Exception as e:\n",
    "        print('exception running get_image_for_leak:',e)\n",
    "        ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parsing tiffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-15T14:20:14.888581Z",
     "start_time": "2017-03-15T22:20:05.294312+08:00"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0198113e0d8e4ec4b4cb6ec322b69624"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3696"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shapely\n",
    "X = []\n",
    "y = []\n",
    "m = []\n",
    "t = []\n",
    "discarded = []\n",
    "for path in tqdm(cache_dir.listdir()):\n",
    "    files = [file.relpath(path) for file in path.listdir() if file.endswith('.tif')]\n",
    "    if files:\n",
    "        # check metadata\n",
    "        try:\n",
    "            metadata = json.load(open(path.joinpath('metadata.json')))\n",
    "        except Exception as e:\n",
    "            print(path, e)\n",
    "            raise(e)\n",
    "        # e.g.\n",
    "        if '_before_' in path.basename():\n",
    "            yy = True\n",
    "        else:\n",
    "            yy = False\n",
    "            \n",
    "        t1 = arrow.get(metadata['image']['properties']['system:time_end']/1000)\n",
    "        t0 = arrow.get(metadata['leak']['features'][0]['properties']['REPO_Date'])\n",
    "        td=t1-t0\n",
    "        tt = td.total_seconds()       \n",
    "        \n",
    "        # load data\n",
    "        data = tifs2np(path,files,bands=bands_s1)\n",
    "        \n",
    "        empty_bands = (data.sum(-1).sum(-1)==0).sum()\n",
    "\n",
    "        # now if the size is wrong let's interp it\n",
    "#         if data.shape[-2]!=pixel_length or data.shape[-1]!=pixel_length:\n",
    "#             data = np.array([sp.misc.imresize(x,size=(pixel_length,pixel_length),interp='cubic', mode='F') for x in data])\n",
    "                  \n",
    "        # lets check we didn't get the edge of an image\n",
    "        bbox = np.array(metadata['image']['properties']['system:footprint']['coordinates'][0])\n",
    "        loc = metadata['leak']['features'][0]['geometry']['coordinates']\n",
    "        minx=bbox[:,0].min()\n",
    "        maxx=bbox[:,0].max()\n",
    "        miny=bbox[:,1].min()\n",
    "        maxy=bbox[:,1].max()\n",
    "        bbox_shp = shapely.geometry.box(\n",
    "            minx=minx,\n",
    "            maxx=maxx,\n",
    "            miny=miny,\n",
    "            maxy=maxy\n",
    "        )\n",
    "        loc_shp = shapely.geometry.Point(loc[0],loc[1])\n",
    "        shapely.geometry.GeometryCollection([bbox_shp, loc_shp])\n",
    "        try:\n",
    "            assert loc_shp.intersects(bbox_shp), 'leak location should be inside image'\n",
    "            assert bbox_shp.centroid.almost_equals(loc_shp, decimal=5), 'leak should be near center of image'\n",
    "#             assert bbox_shp.area>4.5e-06, 'image area should be the right amount of pixels'\n",
    "            assert (maxx-minx)/(maxy-miny)<1.3, 'should be roughly square'\n",
    "            assert (maxx-minx)/(maxy-miny)>0.7, 'should be roughly square'\n",
    "            assert empty_bands<3, 'non qc bands should not be empty'\n",
    "        except Exception as exc:\n",
    "            print(path, exc)\n",
    "            discarded.append(path)\n",
    "        else:\n",
    "            X.append(data)\n",
    "            y.append(yy)\n",
    "            t.append(tt)\n",
    "            m.append(metadata)\n",
    "        \n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-15T14:17:27.457228Z",
     "start_time": "2017-03-15T22:17:27.454284+08:00"
    }
   },
   "outputs": [],
   "source": [
    "# # for broken dirs\n",
    "# path.rename(output_dir.joinpath('deleteme'+str(uuid.uuid4())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-15T04:51:46.348918Z",
     "start_time": "2017-03-15T12:51:46.345954+08:00"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-15T04:49:27.335951Z",
     "start_time": "2017-03-15T12:49:11.775230+08:00"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-15T14:17:30.312451Z",
     "start_time": "2017-03-15T22:17:30.237716+08:00"
    }
   },
   "outputs": [],
   "source": [
    "# shuffle\n",
    "from sklearn.utils import shuffle\n",
    "X,y,m= shuffle(X,y,m,random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-15T14:17:30.842548Z",
     "start_time": "2017-03-15T22:17:30.839548+08:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('../data/20170314-05-26-52_testing_earth_engine-s1-AUTX_v3/ee_S1_AUTX-leaks_cache_v3/70617_before_3857_10.0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-14T05:02:29.841313Z",
     "start_time": "2017-03-14T13:02:29.827607+08:00"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-15T14:17:31.752296Z",
     "start_time": "2017-03-15T22:17:31.749922+08:00"
    }
   },
   "outputs": [],
   "source": [
    "# # which bands do we have?\n",
    "# a=np.array([x.sum(-1).sum(-1)==0 for x in X])\n",
    "# print('amount of each band',list(zip(bands_s1,a.sum(0))))\n",
    "# print('mean amount of bands',a.sum(1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-15T14:17:32.151433Z",
     "start_time": "2017-03-15T22:17:32.142878+08:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({10.0000: 3696})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what resolutions? all 10m, that's good!\n",
    "collections.Counter([mm['image']['properties']['resolution_meters'] for mm in m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-15T14:17:37.426994Z",
     "start_time": "2017-03-15T22:17:35.598608+08:00"
    }
   },
   "outputs": [],
   "source": [
    "# save using hdf5 (so keras can easily load it) and json \n",
    "import h5py\n",
    "h5file = output_dir.joinpath('data.h5')\n",
    "with h5py.File(h5file, 'w') as h5f:\n",
    "    h5f.create_dataset('X', data=X)\n",
    "    h5f.create_dataset('y', data=y)\n",
    "#     h5f.create_dataset('t', data=t)\n",
    "\n",
    "json.dump(m,open(output_dir.joinpath('data_metadata.json'),'w'))\n",
    "\n",
    "with open(output_dir.joinpath('readme.md'),'w') as fo:\n",
    "    fo. write(\"\"\"\n",
    "Files:\n",
    "- ee_S1_AUTX-leaks_cache- cached tiff files\n",
    "- script_metadata.json - information on scraping script\n",
    "- data.h5 contains X, y, and t.\n",
    "    - X: tiff files for each band loaded into an array of shape (Leak, Bands, width, length)\n",
    "    - y: True for before the leak, False for after\n",
    "    - t: time before leak (can be negative) in seconds\n",
    "- data_metadata: array of metadata for each leak in X. Each contain info on leak, image, and image search\n",
    "    \n",
    "Loading: \n",
    "```py\n",
    "# load\n",
    "metadatas = json.load(open('data_metadata.json'))\n",
    "with h5py.File('data.h5','r') as h5f:\n",
    "    X2 = h5f['X'][:]\n",
    "    y2 = h5f['y'][:]\n",
    "    t2 = h5f['t'][:]\n",
    "y\n",
    "```\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-15T14:17:37.804024Z",
     "start_time": "2017-03-15T22:17:37.428448+08:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3696, 5, 25, 25),\n",
       " array([ True, False,  True, ..., False,  True, False], dtype=bool),\n",
       " dict_keys(['name', 'leak', 'image', 'distance', 'scale', 'crs']))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test load\n",
    "metadatas = json.load(open(output_dir.joinpath('data_metadata.json')))\n",
    "with h5py.File(output_dir.joinpath('data.h5'),'r') as h5f:\n",
    "    X2 = h5f['X'][:]\n",
    "    y2 = h5f['y'][:]\n",
    "X2.shape, y2, metadatas[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-14T05:14:58.720471Z",
     "start_time": "2017-03-14T13:14:58.704557+08:00"
    },
    "collapsed": true
   },
   "source": [
    "# test deleteme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-15T14:18:38.223921Z",
     "start_time": "2017-03-15T22:18:36.868549+08:00"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']=\"\" # to disable gpu, so I can do large predictions in memory\n",
    "\n",
    "helper_dir = str(Path('.').abspath())\n",
    "if helper_dir not in os.sys.path:\n",
    "    os.sys.path.append(helper_dir)\n",
    "\n",
    "from leak_helpers.earth_engine import display_ee, get_boundary, tifs2np, bands_s2, download_image, bands_s2\n",
    "from leak_helpers.geometry import diffxy, resample_polygon\n",
    "from leak_helpers.modelling import ImageDataGenerator, dice_coef_loss\n",
    "from leak_helpers.visualization import imshow_bands\n",
    "from leak_helpers.analysis import parse_classification_report, find_best_dummy_classification, calculate_result_class\n",
    "from leak_helpers.modelling.filters import is_not_cloudy, is_not_center_cloudy, is_image_within, is_leak, filter_split_data, is_not_dup, hash_rows, normalise_bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-15T14:18:38.235636Z",
     "start_time": "2017-03-15T22:18:38.225522+08:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "687040.0200"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md=m[2]\n",
    "t_image = arrow.get(md['image']['properties']['system:time_end'] / 1000)\n",
    "t_leak = arrow.get(md['leak']['features'][0]['properties']['REPO_Date'])\n",
    "seconds_before_leak = (t_leak - t_image).total_seconds()\n",
    "seconds_before_leak#<60*60*24*4\n",
    "# [is_image_within(mm, 60*60*24*4) for mm in m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-15T14:21:14.849574Z",
     "start_time": "2017-03-15T22:21:14.806757+08:00"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-15T14:21:20.218074Z",
     "start_time": "2017-03-15T22:21:19.743358+08:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kept 2176 of 3696\n"
     ]
    }
   ],
   "source": [
    "a = has_three_bands = np.array([x.sum(-1).sum(-1)==0 for x in X]).sum(1)>1\n",
    "b = [is_image_within(mm,60*60*24*3) for mm in m]\n",
    "c = is_not_dup(X)\n",
    "keep = a&b&c\n",
    "\n",
    "X = X[keep]\n",
    "y = y[keep]\n",
    "m = [m[i] for i in range(len(m)) if keep[i]]\n",
    "print('kept',keep.sum(),'of',len(keep))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-15T14:21:33.918412Z",
     "start_time": "2017-03-15T22:21:33.894148+08:00"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test, metadata_train, metadata_test = train_test_split(\n",
    "        X, y, m)\n",
    "\n",
    "X_train2 = X_train.reshape((len(X_train),-1))\n",
    "X_test2 = X_test.reshape((len(X_test),-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-15T14:21:37.786035Z",
     "start_time": "2017-03-15T22:21:35.873388+08:00"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isisilon/.virtualenvs/py3syspck/lib/python3.4/site-packages/sklearn/metrics/classification.py:516: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(var_yt * var_yp)\n",
      "/home/isisilon/.virtualenvs/py3syspck/lib/python3.4/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>report</th>\n",
       "      <th>strategy</th>\n",
       "      <th>score</th>\n",
       "      <th>matthews_corrcoef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_stratified</td>\n",
       "      <td>0.675345</td>\n",
       "      <td>0.091104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_stratified</td>\n",
       "      <td>0.690658</td>\n",
       "      <td>0.087100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_uniform</td>\n",
       "      <td>0.493109</td>\n",
       "      <td>0.077541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_uniform</td>\n",
       "      <td>0.486983</td>\n",
       "      <td>0.074884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_stratified</td>\n",
       "      <td>0.675345</td>\n",
       "      <td>0.069670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_stratified</td>\n",
       "      <td>0.683002</td>\n",
       "      <td>0.060724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_uniform</td>\n",
       "      <td>0.502297</td>\n",
       "      <td>0.058087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_uniform</td>\n",
       "      <td>0.502297</td>\n",
       "      <td>0.053772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_uniform</td>\n",
       "      <td>0.520674</td>\n",
       "      <td>0.053003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_uniform</td>\n",
       "      <td>0.491577</td>\n",
       "      <td>0.050116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_stratified</td>\n",
       "      <td>0.692190</td>\n",
       "      <td>0.044330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_uniform</td>\n",
       "      <td>0.493109</td>\n",
       "      <td>0.043624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_stratified</td>\n",
       "      <td>0.705972</td>\n",
       "      <td>0.040883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_uniform</td>\n",
       "      <td>0.516080</td>\n",
       "      <td>0.040737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_uniform</td>\n",
       "      <td>0.514548</td>\n",
       "      <td>0.037872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_uniform</td>\n",
       "      <td>0.499234</td>\n",
       "      <td>0.037180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_uniform</td>\n",
       "      <td>0.465544</td>\n",
       "      <td>0.037136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_uniform</td>\n",
       "      <td>0.479326</td>\n",
       "      <td>0.032898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_uniform</td>\n",
       "      <td>0.511485</td>\n",
       "      <td>0.032898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_uniform</td>\n",
       "      <td>0.519142</td>\n",
       "      <td>0.032089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_stratified</td>\n",
       "      <td>0.666156</td>\n",
       "      <td>0.029690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_stratified</td>\n",
       "      <td>0.687596</td>\n",
       "      <td>0.028164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_uniform</td>\n",
       "      <td>0.509954</td>\n",
       "      <td>0.025600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_stratified</td>\n",
       "      <td>0.732006</td>\n",
       "      <td>0.025245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_uniform</td>\n",
       "      <td>0.474732</td>\n",
       "      <td>0.024155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_stratified</td>\n",
       "      <td>0.678407</td>\n",
       "      <td>0.023778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_uniform</td>\n",
       "      <td>0.506891</td>\n",
       "      <td>0.019317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_uniform</td>\n",
       "      <td>0.502297</td>\n",
       "      <td>0.017642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_stratified</td>\n",
       "      <td>0.681470</td>\n",
       "      <td>0.017362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_uniform</td>\n",
       "      <td>0.535988</td>\n",
       "      <td>0.015565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_uniform</td>\n",
       "      <td>0.526799</td>\n",
       "      <td>-0.022004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_stratified</td>\n",
       "      <td>0.699847</td>\n",
       "      <td>-0.022038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_stratified</td>\n",
       "      <td>0.701378</td>\n",
       "      <td>-0.025366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_stratified</td>\n",
       "      <td>0.716692</td>\n",
       "      <td>-0.025366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_stratified</td>\n",
       "      <td>0.669219</td>\n",
       "      <td>-0.025487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_stratified</td>\n",
       "      <td>0.679939</td>\n",
       "      <td>-0.026730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_uniform</td>\n",
       "      <td>0.464012</td>\n",
       "      <td>-0.027791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_stratified</td>\n",
       "      <td>0.696784</td>\n",
       "      <td>-0.030594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_uniform</td>\n",
       "      <td>0.497703</td>\n",
       "      <td>-0.030644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_stratified</td>\n",
       "      <td>0.705972</td>\n",
       "      <td>-0.030796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_stratified</td>\n",
       "      <td>0.690658</td>\n",
       "      <td>-0.031146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_uniform</td>\n",
       "      <td>0.511485</td>\n",
       "      <td>-0.032135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_stratified</td>\n",
       "      <td>0.681470</td>\n",
       "      <td>-0.032902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_stratified</td>\n",
       "      <td>0.690658</td>\n",
       "      <td>-0.033590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_stratified</td>\n",
       "      <td>0.709035</td>\n",
       "      <td>-0.034647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_stratified</td>\n",
       "      <td>0.696784</td>\n",
       "      <td>-0.034647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_uniform</td>\n",
       "      <td>0.517611</td>\n",
       "      <td>-0.035034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_uniform</td>\n",
       "      <td>0.493109</td>\n",
       "      <td>-0.035689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_stratified</td>\n",
       "      <td>0.709035</td>\n",
       "      <td>-0.039826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_stratified</td>\n",
       "      <td>0.693721</td>\n",
       "      <td>-0.042962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_uniform</td>\n",
       "      <td>0.459418</td>\n",
       "      <td>-0.049402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_uniform</td>\n",
       "      <td>0.494640</td>\n",
       "      <td>-0.050910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_uniform</td>\n",
       "      <td>0.476263</td>\n",
       "      <td>-0.055891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_stratified</td>\n",
       "      <td>0.684533</td>\n",
       "      <td>-0.058648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_stratified</td>\n",
       "      <td>0.702910</td>\n",
       "      <td>-0.061775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_uniform</td>\n",
       "      <td>0.500766</td>\n",
       "      <td>-0.066715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_uniform</td>\n",
       "      <td>0.479326</td>\n",
       "      <td>-0.067579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_stratified</td>\n",
       "      <td>0.676876</td>\n",
       "      <td>-0.068706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_uniform</td>\n",
       "      <td>0.528331</td>\n",
       "      <td>-0.073951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>precision  recall  f1-score  supp...</td>\n",
       "      <td>classifier_stratified</td>\n",
       "      <td>0.681470</td>\n",
       "      <td>-0.081622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                report               strategy  \\\n",
       "207               precision  recall  f1-score  supp...  classifier_stratified   \n",
       "15                precision  recall  f1-score  supp...  classifier_stratified   \n",
       "25                precision  recall  f1-score  supp...     classifier_uniform   \n",
       "157               precision  recall  f1-score  supp...     classifier_uniform   \n",
       "81                precision  recall  f1-score  supp...  classifier_stratified   \n",
       "255               precision  recall  f1-score  supp...  classifier_stratified   \n",
       "235               precision  recall  f1-score  supp...     classifier_uniform   \n",
       "175               precision  recall  f1-score  supp...     classifier_uniform   \n",
       "199               precision  recall  f1-score  supp...     classifier_uniform   \n",
       "133               precision  recall  f1-score  supp...     classifier_uniform   \n",
       "237               precision  recall  f1-score  supp...  classifier_stratified   \n",
       "253               precision  recall  f1-score  supp...     classifier_uniform   \n",
       "231               precision  recall  f1-score  supp...  classifier_stratified   \n",
       "91                precision  recall  f1-score  supp...     classifier_uniform   \n",
       "205               precision  recall  f1-score  supp...     classifier_uniform   \n",
       "229               precision  recall  f1-score  supp...     classifier_uniform   \n",
       "121               precision  recall  f1-score  supp...     classifier_uniform   \n",
       "55                precision  recall  f1-score  supp...     classifier_uniform   \n",
       "259               precision  recall  f1-score  supp...     classifier_uniform   \n",
       "151               precision  recall  f1-score  supp...     classifier_uniform   \n",
       "189               precision  recall  f1-score  supp...  classifier_stratified   \n",
       "177               precision  recall  f1-score  supp...  classifier_stratified   \n",
       "277               precision  recall  f1-score  supp...     classifier_uniform   \n",
       "57                precision  recall  f1-score  supp...  classifier_stratified   \n",
       "61                precision  recall  f1-score  supp...     classifier_uniform   \n",
       "141               precision  recall  f1-score  supp...  classifier_stratified   \n",
       "1                 precision  recall  f1-score  supp...     classifier_uniform   \n",
       "103               precision  recall  f1-score  supp...     classifier_uniform   \n",
       "117               precision  recall  f1-score  supp...  classifier_stratified   \n",
       "109               precision  recall  f1-score  supp...     classifier_uniform   \n",
       "..                                                 ...                    ...   \n",
       "271               precision  recall  f1-score  supp...     classifier_uniform   \n",
       "21                precision  recall  f1-score  supp...  classifier_stratified   \n",
       "111               precision  recall  f1-score  supp...  classifier_stratified   \n",
       "195               precision  recall  f1-score  supp...  classifier_stratified   \n",
       "99                precision  recall  f1-score  supp...  classifier_stratified   \n",
       "273               precision  recall  f1-score  supp...  classifier_stratified   \n",
       "37                precision  recall  f1-score  supp...     classifier_uniform   \n",
       "225               precision  recall  f1-score  supp...  classifier_stratified   \n",
       "145               precision  recall  f1-score  supp...     classifier_uniform   \n",
       "171               precision  recall  f1-score  supp...  classifier_stratified   \n",
       "63                precision  recall  f1-score  supp...  classifier_stratified   \n",
       "115               precision  recall  f1-score  supp...     classifier_uniform   \n",
       "243               precision  recall  f1-score  supp...  classifier_stratified   \n",
       "51                precision  recall  f1-score  supp...  classifier_stratified   \n",
       "33                precision  recall  f1-score  supp...  classifier_stratified   \n",
       "135               precision  recall  f1-score  supp...  classifier_stratified   \n",
       "73                precision  recall  f1-score  supp...     classifier_uniform   \n",
       "43                precision  recall  f1-score  supp...     classifier_uniform   \n",
       "75                precision  recall  f1-score  supp...  classifier_stratified   \n",
       "27                precision  recall  f1-score  supp...  classifier_stratified   \n",
       "181               precision  recall  f1-score  supp...     classifier_uniform   \n",
       "265               precision  recall  f1-score  supp...     classifier_uniform   \n",
       "31                precision  recall  f1-score  supp...     classifier_uniform   \n",
       "213               precision  recall  f1-score  supp...  classifier_stratified   \n",
       "105               precision  recall  f1-score  supp...  classifier_stratified   \n",
       "187               precision  recall  f1-score  supp...     classifier_uniform   \n",
       "67                precision  recall  f1-score  supp...     classifier_uniform   \n",
       "291               precision  recall  f1-score  supp...  classifier_stratified   \n",
       "163               precision  recall  f1-score  supp...     classifier_uniform   \n",
       "147               precision  recall  f1-score  supp...  classifier_stratified   \n",
       "\n",
       "        score  matthews_corrcoef  \n",
       "207  0.675345           0.091104  \n",
       "15   0.690658           0.087100  \n",
       "25   0.493109           0.077541  \n",
       "157  0.486983           0.074884  \n",
       "81   0.675345           0.069670  \n",
       "255  0.683002           0.060724  \n",
       "235  0.502297           0.058087  \n",
       "175  0.502297           0.053772  \n",
       "199  0.520674           0.053003  \n",
       "133  0.491577           0.050116  \n",
       "237  0.692190           0.044330  \n",
       "253  0.493109           0.043624  \n",
       "231  0.705972           0.040883  \n",
       "91   0.516080           0.040737  \n",
       "205  0.514548           0.037872  \n",
       "229  0.499234           0.037180  \n",
       "121  0.465544           0.037136  \n",
       "55   0.479326           0.032898  \n",
       "259  0.511485           0.032898  \n",
       "151  0.519142           0.032089  \n",
       "189  0.666156           0.029690  \n",
       "177  0.687596           0.028164  \n",
       "277  0.509954           0.025600  \n",
       "57   0.732006           0.025245  \n",
       "61   0.474732           0.024155  \n",
       "141  0.678407           0.023778  \n",
       "1    0.506891           0.019317  \n",
       "103  0.502297           0.017642  \n",
       "117  0.681470           0.017362  \n",
       "109  0.535988           0.015565  \n",
       "..        ...                ...  \n",
       "271  0.526799          -0.022004  \n",
       "21   0.699847          -0.022038  \n",
       "111  0.701378          -0.025366  \n",
       "195  0.716692          -0.025366  \n",
       "99   0.669219          -0.025487  \n",
       "273  0.679939          -0.026730  \n",
       "37   0.464012          -0.027791  \n",
       "225  0.696784          -0.030594  \n",
       "145  0.497703          -0.030644  \n",
       "171  0.705972          -0.030796  \n",
       "63   0.690658          -0.031146  \n",
       "115  0.511485          -0.032135  \n",
       "243  0.681470          -0.032902  \n",
       "51   0.690658          -0.033590  \n",
       "33   0.709035          -0.034647  \n",
       "135  0.696784          -0.034647  \n",
       "73   0.517611          -0.035034  \n",
       "43   0.493109          -0.035689  \n",
       "75   0.709035          -0.039826  \n",
       "27   0.693721          -0.042962  \n",
       "181  0.459418          -0.049402  \n",
       "265  0.494640          -0.050910  \n",
       "31   0.476263          -0.055891  \n",
       "213  0.684533          -0.058648  \n",
       "105  0.702910          -0.061775  \n",
       "187  0.500766          -0.066715  \n",
       "67   0.479326          -0.067579  \n",
       "291  0.676876          -0.068706  \n",
       "163  0.528331          -0.073951  \n",
       "147  0.681470          -0.081622  \n",
       "\n",
       "[300 rows x 4 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_names = ['noleak','leak']\n",
    "df_dummies, best_dummy = find_best_dummy_classification(X,y,n=50, target_names=target_names)\n",
    "df_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-15T14:23:15.808144Z",
     "start_time": "2017-03-15T22:23:03.106929+08:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "     noleak       0.79      1.00      0.88       429\n",
      "       leak       0.00      0.00      0.00       115\n",
      "\n",
      "avg / total       0.62      0.79      0.70       544\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isisilon/.virtualenvs/py3syspck/lib/python3.4/site-packages/sklearn/metrics/classification.py:516: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(var_yt * var_yp)\n",
      "/home/isisilon/.virtualenvs/py3syspck/lib/python3.4/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7886, 0.0000)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.ensemble\n",
    "thresh=0.5\n",
    "clf = sklearn.ensemble.RandomForestClassifier(\n",
    "    n_estimators=200, \n",
    "    criterion='entropy',\n",
    "#     max_depth=None, \n",
    "    min_samples_split=6, \n",
    "    min_samples_leaf=6,\n",
    "#     max_features='auto', \n",
    "    bootstrap=True,\n",
    "#         random_state=0,\n",
    "#     n_jobs=4, \n",
    ")\n",
    "\n",
    "clf.fit(X_train2, y_train) \n",
    "\n",
    "y_pred = clf.predict(X_test2)\n",
    "score = clf.score(X_test2, y_test)\n",
    "\n",
    "matthews_corrcoef = sklearn.metrics.matthews_corrcoef(y_test>thresh, y_pred>thresh)\n",
    "print(sklearn.metrics.classification_report(y_test > thresh, y_pred > thresh, target_names=target_names))\n",
    "score,matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (with sys packages)",
   "language": "python",
   "name": "py3syspck"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  },
  "toc": {
   "nav_menu": {
    "height": "96px",
    "width": "251px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
