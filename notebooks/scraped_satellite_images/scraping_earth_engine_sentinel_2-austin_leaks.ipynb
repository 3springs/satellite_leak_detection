{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook scrapes satellite images for each leak repair. For each location it gets a NxM rectangle around the leak before and after it was repaired. Then it collated all the data into h5 files and all the metadata into json files.\n",
    "\n",
    "It takes days to run because of rate limiting on the google earth api. Because of limited satelite coverage you might find matches for only 10% of the leaks.\n",
    "\n",
    "## Modifying\n",
    "\n",
    "- make sure google earth is setup\n",
    "- load leaks, so they pass the asserts\n",
    "- change params\n",
    "- run rest of cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-16T06:09:41.986296Z",
     "start_time": "2017-03-16T14:09:40.699962+08:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%.4f'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from path import Path\n",
    "import arrow\n",
    "import json\n",
    "import pytz\n",
    "from pprint import pprint\n",
    "from tqdm import tnrange, tqdm_notebook as tqdm\n",
    "import re, os, collections, itertools, uuid, logging\n",
    "import tempfile\n",
    "import tables\n",
    "import shapely\n",
    "\n",
    "import zipfile\n",
    "import urllib\n",
    "\n",
    "import ee\n",
    "import pyproj\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (15, 5) # bigger plots\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "%precision 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-16T06:09:45.232506Z",
     "start_time": "2017-03-16T14:09:42.052595+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "helper_dir = str(Path('..').abspath())\n",
    "if helper_dir not in os.sys.path:\n",
    "    os.sys.path.append(helper_dir)\n",
    "    \n",
    "from leak_helpers.earth_engine import display_ee, get_boundary, tifs2np, bands_s2, download_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-16T06:09:45.237820Z",
     "start_time": "2017-03-16T14:09:45.234693+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-06T13:43:10.906470",
     "start_time": "2017-03-06T13:43:10.867216"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-16T06:09:45.267733Z",
     "start_time": "2017-03-16T14:09:45.240556+08:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Path('/tmp/image_testing_earth_engine_s2-AUTX_v6'),\n",
       " Path('../data/downloaded_images_image_testing_earth_engine_s2-AUTX_v6_COPERNICUS-S2'),\n",
       " Path('../data/downloaded_images_image_testing_earth_engine_s2-AUTX_v6_COPERNICUS-S2/cache'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# params\n",
    "crs_grid = 3857 # texas central meters\n",
    "satellite = 'COPERNICUS/S2'\n",
    "notebook_name='image_testing_earth_engine_s2-AUTX_v6'\n",
    "ts=arrow.utcnow().format('YYYYMMDD-HH-mm-ss')\n",
    "data_dir = Path('../../data/')\n",
    "bands = bands_s2\n",
    "\n",
    "# since the lowest res band is 60m and I want to capture neighbours I should get 6+ pixels\n",
    "pixel_length = 25.0\n",
    "resolution_min = 10.0 # m\n",
    "time_bin_delta = 60*60*24*28 # how long before a leak to look (in seconds)\n",
    "# TODO get closest but let me filter for time\n",
    "\n",
    "# init\n",
    "# temp_dir = Path(tempfile.mkdtemp(prefix=notebook_name+'-', suffix='-'+ts))\n",
    "temp_dir = Path('/tmp/{}'.format(notebook_name))\n",
    "output_dir = Path('../../data/scraped_satellite_images/downloaded_images_{}_{}'.format(notebook_name,satellite.replace('/','-')))\n",
    "cache_dir = output_dir.joinpath('cache')\n",
    "output_dir.makedirs_p()\n",
    "temp_dir.makedirs_p()\n",
    "cache_dir.makedirs_p()\n",
    "\n",
    "logger = logging.getLogger(notebook_name)\n",
    "# logger.setLevel(logging.WARN)\n",
    "\n",
    "crs_grid_proj = pyproj.Proj('+init=epsg:%s'%crs_grid)\n",
    "\n",
    "temp_dir, output_dir, cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-17T14:30:44.254121",
     "start_time": "2017-01-17T14:30:44.250954"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-16T06:09:45.280270Z",
     "start_time": "2017-03-16T14:09:45.270765+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metadata_file = output_dir.joinpath('script_metadata.json')\n",
    "\n",
    "# write metadata to json\n",
    "metadata = dict(\n",
    "    pixel_length=pixel_length,\n",
    "    resolution_min=resolution_min,\n",
    "    bands=bands,\n",
    "    ts=ts,\n",
    "    notebook_name=notebook_name,\n",
    "    crs_grid=crs_grid,\n",
    "    cache_dir=str(cache_dir),\n",
    "    temp_dir=str(temp_dir),\n",
    "    output_dir=str(output_dir),\n",
    ")\n",
    "json.dump(metadata, open(metadata_file,'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# earth engine\n",
    "\n",
    "Setup instructions here\n",
    "- first need to apply for an account and wait ~ 1day\n",
    "- https://developers.google.com/earth-engine/python_install#setting-up-authentication-credentials\n",
    "\n",
    "Refs:\n",
    "- api https://developers.google.com/earth-engine/\n",
    "- code examples https://code.earthengine.google.com/\n",
    "- sentinel1 https://developers.google.com/earth-engine/sentinel1\n",
    "    - `ee.ImageCollection('COPERNICUS/S2_GRD');`\n",
    "    - `ee.ImageCollection('COPERNICUS/S1_GRD');`\n",
    "- keras and google earth https://github.com/patrick-dd/landsat-landstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-16T06:09:52.672612Z",
     "start_time": "2017-03-16T14:09:45.283572+08:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "# test earth-engine setup\n",
    "from oauth2client import crypt # should have not error\n",
    "import ee\n",
    "ee.Initialize() # should give no errors, if so follow instructions\n",
    "\n",
    "\n",
    "# test\n",
    "image = ee.Image('srtm90_v4')\n",
    "assert image.getInfo()=={'type': 'Image', 'properties': {'system:time_start': 950227200000, 'system:asset_size': 18827626666, 'system:time_end': 951177600000}, 'bands': [{'data_type': {'type': 'PixelType', 'max': 32767, 'min': -32768, 'precision': 'int'}, 'crs': 'EPSG:4326', 'id': 'elevation', 'dimensions': [432000, 144000], 'crs_transform': [0.000833333333333, 0.0, -180.0, 0.0, -0.000833333333333, 60.0]}], 'id': 'srtm90_v4', 'version': 1463778555689000}\n",
    "print('ok')\n",
    "\n",
    "# ee.Geometry.Point([117.21079620254062, -30.94712385398404])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load leaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-06T13:48:10.216232",
     "start_time": "2017-03-06T13:48:10.124184"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-16T06:09:57.143612Z",
     "start_time": "2017-03-16T14:09:52.674609+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load wa leaks\n",
    "leaks_ATX = gpd.read_file(data_dir.joinpath('leak_datasets/austin_leaks/derived/austin_leaks-repairs.geojson'))\n",
    "\n",
    "\n",
    "# they have to be after launch\n",
    "leaks_ATX = leaks_ATX[pd.to_datetime(leaks_ATX.COMPDTTM)>=pd.Timestamp('23 June 2015')]\n",
    "len(leaks_ATX)\n",
    "\n",
    "leaks_ATX['REPO_Date']=leaks_ATX['COMPDTTM']\n",
    "leaks_ATX['leak_id']=leaks_ATX.OBJECTID.apply(lambda x:'ATX-%s'%x)\n",
    "leaks=leaks_ATX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-16T06:09:57.189917Z",
     "start_time": "2017-03-16T14:09:57.146136+08:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>22</th>\n",
       "      <th>ADDRKEY</th>\n",
       "      <th>CITY</th>\n",
       "      <th>COMPDTTM</th>\n",
       "      <th>DESCRIPT</th>\n",
       "      <th>FullStreetName</th>\n",
       "      <th>INITDTTM</th>\n",
       "      <th>LOC</th>\n",
       "      <th>OBJECTID</th>\n",
       "      <th>PREDIR</th>\n",
       "      <th>...</th>\n",
       "      <th>STNAME</th>\n",
       "      <th>STNO</th>\n",
       "      <th>STSUB</th>\n",
       "      <th>SUFFIX</th>\n",
       "      <th>WONO</th>\n",
       "      <th>ZIP</th>\n",
       "      <th>geometry</th>\n",
       "      <th>id</th>\n",
       "      <th>REPO_Date</th>\n",
       "      <th>leak_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22152</th>\n",
       "      <td>510083.0</td>\n",
       "      <td>608224.0</td>\n",
       "      <td>AUSTIN</td>\n",
       "      <td>2016-07-25T21:00:00</td>\n",
       "      <td>WATER SERVICE LEAK</td>\n",
       "      <td>2711 HILLVIEW GREEN LN</td>\n",
       "      <td>2016-07-25T21:00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>69638</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>HILLVIEW GREEN</td>\n",
       "      <td>2711</td>\n",
       "      <td></td>\n",
       "      <td>LN</td>\n",
       "      <td>1767141.0</td>\n",
       "      <td>78703-</td>\n",
       "      <td>POINT (-97.76605986741285 30.30257718194047)</td>\n",
       "      <td>69638</td>\n",
       "      <td>2016-07-25T21:00:00</td>\n",
       "      <td>ATX-69638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             22   ADDRKEY    CITY             COMPDTTM            DESCRIPT  \\\n",
       "22152  510083.0  608224.0  AUSTIN  2016-07-25T21:00:00  WATER SERVICE LEAK   \n",
       "\n",
       "               FullStreetName             INITDTTM   LOC  OBJECTID PREDIR  \\\n",
       "22152  2711 HILLVIEW GREEN LN  2016-07-25T21:00:00  None     69638          \n",
       "\n",
       "         ...              STNAME  STNO  STSUB SUFFIX       WONO         ZIP  \\\n",
       "22152    ...      HILLVIEW GREEN  2711            LN  1767141.0  78703-       \n",
       "\n",
       "                                           geometry     id  \\\n",
       "22152  POINT (-97.76605986741285 30.30257718194047)  69638   \n",
       "\n",
       "                 REPO_Date    leak_id  \n",
       "22152  2016-07-25T21:00:00  ATX-69638  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose one leak for now\n",
    "leak = leaks.sample()\n",
    "leak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-15T10:46:31.548298",
     "start_time": "2017-01-15T10:46:31.546367"
    }
   },
   "source": [
    "# Fetching sentinal-1 and sentinel 2 images\n",
    "\n",
    "For a leak repair, grab the image before and after it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note roughly 10% have results for a 1 day temporal bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-16T06:09:57.202371Z",
     "start_time": "2017-03-16T14:09:57.193264+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cached_ids():\n",
    "    cache_dirs = [str(f.relpath(cache_dir)).split('_')[0] for f in cache_dir.listdir()]\n",
    "    return cache_dirs\n",
    "\n",
    "def init_cache(leak_id):\n",
    "    \"\"\"We will cache downloads in folders like 'id_after'\"\"\"\n",
    "    if leak_id:\n",
    "        cache_subdir = cache_dir.joinpath(leak_id+'_after')\n",
    "        cache_subdir.makedirs_p()\n",
    "        cache_subdir = cache_dir.joinpath(leak_id+'_before')\n",
    "        cache_subdir.makedirs_p()\n",
    "    return get_cached_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each point\n",
    "- find the nearest image before the repair\n",
    "- and the soonest image after repair\n",
    "- save a part of each with metadata\n",
    "\n",
    "Later we can filter, interpolate, and read into numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-16T06:09:57.218046Z",
     "start_time": "2017-03-16T14:09:57.205020+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distance = resolution_min*(pixel_length/2.0-0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-16T06:12:03.362209Z",
     "start_time": "2017-03-16T14:09:57.222869+08:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1405 [((25, 25), 928118.0), ((25, 25), 789939.0), ((25, 25), 728441.0), ((25, 25), 705401.0), ((25, 25), 788108.0), ((25, 25), 1023536.0), ((25, 25), 1130338.0), ((25, 25), 1082823.0), ((25, 25), 1213326.0), ((25, 25), 413321.0), ((25, 25), 11618.0), ((25, 25), 1051113.0), ((25, 25), 759413.0), ((25, 25), 0.0)]\n",
      "663 [((25, 25), 820624.0), ((25, 25), 662623.0), ((25, 25), 588262.0), ((25, 25), 525886.0), ((25, 25), 635569.0), ((25, 25), 947381.0), ((25, 25), 1085808.0), ((25, 25), 1065084.0), ((25, 25), 1191142.0), ((25, 25), 428699.0), ((25, 25), 10894.0), ((25, 25), 935971.0), ((25, 25), 635933.0), ((25, 25), 0.0)]\n",
      "139 [((25, 25), 807594.0), ((25, 25), 647622.0), ((25, 25), 578721.0), ((25, 25), 503815.0), ((25, 25), 617403.0), ((25, 25), 972375.0), ((25, 25), 1109169.0), ((25, 25), 1083035.0), ((25, 25), 1219875.0), ((25, 25), 444426.0), ((25, 25), 15954.0), ((25, 25), 957910.0), ((25, 25), 644383.0), ((25, 25), 0.0)]\n",
      "73 [((25, 25), 886434.0), ((25, 25), 755142.0), ((25, 25), 714171.0), ((25, 25), 669074.0), ((25, 25), 803531.0), ((25, 25), 1158232.0), ((25, 25), 1289604.0), ((25, 25), 1250708.0), ((25, 25), 1393705.0), ((25, 25), 475962.0), ((25, 25), 19270.0), ((25, 25), 1236359.0), ((25, 25), 890099.0), ((25, 25), 0.0)]\n",
      "1044 [((25, 25), 793971.0), ((25, 25), 662205.0), ((25, 25), 606745.0), ((25, 25), 527889.0), ((25, 25), 669873.0), ((25, 25), 1048008.0), ((25, 25), 1181330.0), ((25, 25), 1173071.0), ((25, 25), 1287645.0), ((25, 25), 454147.0), ((25, 25), 21199.0), ((25, 25), 991728.0), ((25, 25), 659135.0), ((25, 25), 0.0)]\n"
     ]
    }
   ],
   "source": [
    "# test with one image\n",
    "for i in (np.random.sample(5)*len(leaks)).astype(np.int):\n",
    "    leak=leaks_ATX.iloc[[i]]\n",
    "    leak_id = str(leak.OBJECTID.values[0])\n",
    "\n",
    "    repo_date_ts = arrow.get(leak.REPO_Date.values[0]).timestamp\n",
    "    boundary = get_boundary(leak, distance=distance)\n",
    "    sentinel2_before = ee.ImageCollection(satellite)\\\n",
    "        .filterBounds(boundary)\\\n",
    "        .filterMetadata('CLOUDY_PIXEL_PERCENTAGE','less_than',30)\\\n",
    "        .filterDate(933828614605,1488776737937)\\\n",
    "        .sort('system:time_start', opt_ascending=False) # first will be latest\n",
    "    image = ee.Image(sentinel2_before.first()).clip(boundary)\n",
    "    image.getInfo()\n",
    "    name=leak_id+'_after'\n",
    "    path,files=download_image(\n",
    "        image, \n",
    "        scale=resolution_min, \n",
    "        crs=crs_grid, \n",
    "        name=name,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "    data = tifs2np(path,files,bands=bands)\n",
    "    print(i,[(d.shape,d.sum()) for d in data])\n",
    "    for d in data:\n",
    "        assert d.shape[0]==pixel_length, 'the downloaded image is the wrong size, tweak distance'\n",
    "        assert d.shape[1]==pixel_length\n",
    "    assert np.sum(data)!=0, 'should not be empty (make sure you are using the right bands)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-03-17T09:34:47.554Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf146f7baab843caa9f34674b7bdfa19"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cached_ids = get_cached_ids()\n",
    "\n",
    "# logger = logging.getLogger()\n",
    "# logger.setLevel(logging.WARN)\n",
    "\n",
    "# for i in tqdm(range(len(leaks))):\n",
    "def get_image_for_leak(i, cached_ids=cached_ids):    \n",
    "    leak = leaks_ATX.iloc[[i]]\n",
    "    repo_date_ts = arrow.get(leak.COMPDTTM.values[0]).timestamp\n",
    "    \n",
    "    \n",
    "    # crappy way or recording that we tried this one\n",
    "    leak_id = str(leak.OBJECTID.values[0])\n",
    "    if leak_id in cached_ids:\n",
    "        logger.info('Skipping cached download for leak id %s ',leak_id)\n",
    "        return\n",
    "    \n",
    "    boundary = get_boundary(leak, distance=distance) #, epsg=crs_grid)\n",
    "    \n",
    "    \n",
    "    # get image day before    \n",
    "    sentinel2_before = ee.ImageCollection('COPERNICUS/S2')\\\n",
    "        .filterBounds(boundary)\\\n",
    "        .filterDate((repo_date_ts-time_bin_delta)*1000,(repo_date_ts)*1000)\\\n",
    "        .filterMetadata('CLOUDY_PIXEL_PERCENTAGE','less_than',30)\\\n",
    "        .sort('system:time_start', opt_ascending=False) # first will be latest\n",
    "    \n",
    "    results = sentinel2_before.size().getInfo()\n",
    "    if results<1:\n",
    "        logger.info('Error no results for day before %s',leak_id)\n",
    "        cached_ids = init_cache(leak_id) # so we know there where no results\n",
    "        return\n",
    "        \n",
    "    # get image day after\n",
    "    sentinel2_after = ee.ImageCollection('COPERNICUS/S2')\\\n",
    "        .filterBounds(boundary)\\\n",
    "        .filterDate((repo_date_ts)*1000,(repo_date_ts+time_bin_delta*6)*1000)\\\n",
    "        .filterMetadata('CLOUDY_PIXEL_PERCENTAGE','less_than',30)\\\n",
    "        .sort('system:time_start', opt_ascending=True) # first will be earliest\n",
    "        \n",
    "    results = sentinel2_after.size().getInfo()\n",
    "    if results<1:\n",
    "        logger.info('Error no results for day after, id %s',leak_id)\n",
    "        cached_ids = init_cache(leak_id) # so we know there where no results\n",
    "        return\n",
    "        \n",
    "    # download as save images    \n",
    "    logger.info('results for %s', leak_id)\n",
    "    image = ee.Image(sentinel2_before.first()).clip(boundary)\n",
    "    name=leak_id+'_before'\n",
    "    path,files=download_image(\n",
    "        image, \n",
    "        scale=resolution_min, \n",
    "        crs=crs_grid, \n",
    "        name=name,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "    # also save metadata so we can filter by date\n",
    "    with open(path.joinpath('metadata.json'), 'w') as fo:\n",
    "        metadata = dict(\n",
    "            image=image.getInfo(),\n",
    "            scale=resolution_min,\n",
    "            crs=crs_grid,\n",
    "            name=name,\n",
    "            distance=distance,\n",
    "            leak=json.loads(leak.to_json())\n",
    "        )\n",
    "        json.dump(metadata, fo)\n",
    "\n",
    "    image = ee.Image(sentinel2_after.first()).clip(boundary)\n",
    "    name=leak_id+'_after'\n",
    "    path,files=download_image(\n",
    "        image, \n",
    "        scale=resolution_min, \n",
    "        crs=crs_grid, \n",
    "        name=name,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "    with open(path.joinpath('metadata.json'), 'w') as fo:\n",
    "        metadata = dict(\n",
    "            image=image.getInfo(),\n",
    "            scale=resolution_min,\n",
    "            crs=crs_grid,\n",
    "            name=name,\n",
    "            distance=distance,\n",
    "            leak=json.loads(leak.to_json())\n",
    "        )\n",
    "        json.dump(metadata, fo)\n",
    "        \n",
    "    return\n",
    "\n",
    "for i in tqdm(range(len(leaks_ATX))):\n",
    "# for i in tqdm(range(3480-10,3480)):\n",
    "    try:\n",
    "        get_image_for_leak(i)\n",
    "    except urllib.error.HTTPError as e:\n",
    "        print(i,e,'sleep')\n",
    "        if e.code == 429:\n",
    "             time.sleep(13);\n",
    "    except Exception as e:\n",
    "        print(i,e)\n",
    "        ee.Initialize() # should give no errors, if so follow instructions\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-16T09:49:18.160135Z",
     "start_time": "2017-03-16T17:49:18.142489+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load tiffs to arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-16T06:01:47.524894Z",
     "start_time": "2017-03-16T14:01:47.521876+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-03-17T09:34:48.282Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23264d01a94744758cc06982e6e2ac52"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid metadata.json, deleted folder ../data/downloaded_images_image_testing_earth_engine_s2-AUTX_v6_COPERNICUS-S2/cache/.deleteme-4fe1c440-d4db-472c-a06e-82c22095ef80, please rerun scraping cell to rescrape this image\n",
      "Invalid metadata.json, deleted folder ../data/downloaded_images_image_testing_earth_engine_s2-AUTX_v6_COPERNICUS-S2/cache/.deleteme-9bbfcf72-c2a7-4806-8350-8a5eadd2b7e4, please rerun scraping cell to rescrape this image\n",
      "Invalid metadata.json, deleted folder ../data/downloaded_images_image_testing_earth_engine_s2-AUTX_v6_COPERNICUS-S2/cache/.deleteme-ba39cc06-fa6b-4c55-81c0-1d01ce78563a, please rerun scraping cell to rescrape this image\n",
      "Invalid metadata.json, deleted folder ../data/downloaded_images_image_testing_earth_engine_s2-AUTX_v6_COPERNICUS-S2/cache/.deleteme-d8f62bac-7fad-4254-ae31-72552c895208, please rerun scraping cell to rescrape this image\n",
      "Invalid metadata.json, deleted folder ../data/downloaded_images_image_testing_earth_engine_s2-AUTX_v6_COPERNICUS-S2/cache/.deleteme-f37c3a4b-d709-4bcb-9969-d437f12aeee2, please rerun scraping cell to rescrape this image\n",
      "Invalid metadata.json, deleted folder ../data/downloaded_images_image_testing_earth_engine_s2-AUTX_v6_COPERNICUS-S2/cache/.deleteme-fa3d8803-4d81-4194-9646-9edc35adf7bf, please rerun scraping cell to rescrape this image\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/downloaded_images_image_testing_earth_engine_s2-AUTX_v6_COPERNICUS-S2/cache/67111_before_3857_10.0 should not have all bands empty\n",
      "../data/downloaded_images_image_testing_earth_engine_s2-AUTX_v6_COPERNICUS-S2/cache/69704_before_3857_10.0 leak location should be inside image\n",
      "../data/downloaded_images_image_testing_earth_engine_s2-AUTX_v6_COPERNICUS-S2/cache/68023_before_3857_10.0 should not have all bands empty\n",
      "../data/downloaded_images_image_testing_earth_engine_s2-AUTX_v6_COPERNICUS-S2/cache/68292_after_3857_10.0 leak location should be inside image\n",
      "../data/downloaded_images_image_testing_earth_engine_s2-AUTX_v6_COPERNICUS-S2/cache/68292_before_3857_10.0 leak location should be inside image\n",
      "../data/downloaded_images_image_testing_earth_engine_s2-AUTX_v6_COPERNICUS-S2/cache/68539_after_3857_10.0 leak should be near center of image\n",
      "../data/downloaded_images_image_testing_earth_engine_s2-AUTX_v6_COPERNICUS-S2/cache/68539_before_3857_10.0 leak should be near center of image\n",
      "../data/downloaded_images_image_testing_earth_engine_s2-AUTX_v6_COPERNICUS-S2/cache/68617_before_3857_10.0 leak location should be inside image\n",
      "../data/downloaded_images_image_testing_earth_engine_s2-AUTX_v6_COPERNICUS-S2/cache/68732_after_3857_10.0 leak should be near center of image\n"
     ]
    }
   ],
   "source": [
    "# This loads it as X and y for machine learning, and also time and metadata so we can filter\n",
    "import shapely\n",
    "X = []\n",
    "y = []\n",
    "t = []\n",
    "m = []\n",
    "discarded=[]\n",
    "for path in tqdm(cache_dir.listdir()):\n",
    "    files = [file.relpath(path) for file in path.listdir() if file.endswith('.tif')]\n",
    "    if files:\n",
    "        # check metadata\n",
    "        try:\n",
    "            metadata = json.load(open(path.joinpath('metadata.json')))\n",
    "        except (FileNotFoundError, ValueError) as e:\n",
    "            path.move(path.replace(path.basename(),'.deleteme-'+str(uuid.uuid4())))\n",
    "            if '_after_' in path: # also delete the before path                \n",
    "                path_after = Path(path.replace('_after_','_before_'))\n",
    "                if path_after.isdir():\n",
    "                    path_after.move(path.replace(path.basename(),'.deleteme-'+str(uuid.uuid4())))\n",
    "            logger.error('Invalid metadata.json, deleted folder %s, please rerun scraping cell to rescrape this image', path)\n",
    "            continue\n",
    "        \n",
    "        # e.g. lets filter it so \"before\" image are only 1 day before\n",
    "        if '_before_' in path.basename():\n",
    "            yy = True\n",
    "        else:\n",
    "            yy = False\n",
    "        \n",
    "        # work out time gap too\n",
    "        t1 = arrow.get(metadata['image']['properties']['system:time_end']/1000)\n",
    "        t0 = arrow.get(metadata['leak']['features'][0]['properties']['REPO_Date'])\n",
    "        td=t1-t0\n",
    "        tt = td.total_seconds()\n",
    "        \n",
    "        # load data\n",
    "        data = tifs2np(path,files,bands=bands)\n",
    "             \n",
    "        # check we don't have empty bands 1-13\n",
    "        empty_bands = np.array([d.sum() for d in data])==0\n",
    "        \n",
    "        # lets check we didn't get the edge of an image\n",
    "        bbox = np.array(metadata['image']['properties']['system:footprint']['coordinates'][0])\n",
    "        loc = metadata['leak']['features'][0]['geometry']['coordinates']\n",
    "        minx=bbox[:,0].min()\n",
    "        maxx=bbox[:,0].max()\n",
    "        miny=bbox[:,1].min()\n",
    "        maxy=bbox[:,1].max()\n",
    "        bbox_shp = shapely.geometry.box(\n",
    "            minx=minx,\n",
    "            maxx=maxx,\n",
    "            miny=miny,\n",
    "            maxy=maxy\n",
    "        )\n",
    "        loc_shp = shapely.geometry.Point(loc[0],loc[1])\n",
    "        shapely.geometry.GeometryCollection([bbox_shp, loc_shp])\n",
    "        try:\n",
    "            assert loc_shp.intersects(bbox_shp), 'leak location should be inside image'\n",
    "            assert bbox_shp.centroid.almost_equals(loc_shp, decimal=5), 'leak should be near center of image'\n",
    "            assert (np.array([d.shape for d in data])==pixel_length).all(), 'image area should be the right amount of pixels'\n",
    "            assert (maxx-minx)/(maxy-miny)<1.3, 'should be roughly square'\n",
    "            assert (maxx-minx)/(maxy-miny)>0.7, 'should be roughly square'\n",
    "            assert not empty_bands.all(), 'should not have all bands empty'\n",
    "        except Exception as exc:\n",
    "            print(path, exc)\n",
    "#             raise(exc)\n",
    "            discarded.append(path)\n",
    "        else:\n",
    "            X.append(data)\n",
    "            y.append(yy)\n",
    "            t.append(tt)\n",
    "            m.append(metadata)\n",
    "        \n",
    "\n",
    "len(X), len(discarded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-16T06:00:35.695987Z",
     "start_time": "2017-03-16T14:00:35.692755+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-03-17T09:34:48.665Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some leaks say REPO_data some say COMPDTTM I'll unify them\n",
    "for mm in m:\n",
    "    props = mm['leak']['features'][0]['properties']\n",
    "    if 'REPO_Date' not in props:\n",
    "        props['REPO_Date']=props['COMPDTTM']\n",
    "    if 'leak_id' not in props:\n",
    "        props['leak_id']='AU_%s'%props['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-03-17T09:34:48.825Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # if there is an error rename the dir\n",
    "# 2497 2739\n",
    "# newname = path.basename().replace('_','-deleteme-')\n",
    "# newpath=Path('/tmp/').joinpath(newname)\n",
    "# print(path.basename(),newname)\n",
    "# path.rename(newname)\n",
    "# path.move(newpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-14T02:00:49.938821Z",
     "start_time": "2017-03-14T10:00:49.934326+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-03-17T09:34:49.121Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shuffle\n",
    "from sklearn.utils import shuffle\n",
    "X,y,m,t = shuffle(X,y,m,t,random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-03-17T09:34:49.345Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save using hdf5 (so keras can easily load it) and json \n",
    "import h5py\n",
    "h5file = output_dir.joinpath('data.h5')\n",
    "with h5py.File(h5file, 'w') as h5f:\n",
    "    h5f.create_dataset('X', data=X)\n",
    "    h5f.create_dataset('y', data=y)\n",
    "    h5f.create_dataset('t', data=t)\n",
    "\n",
    "json.dump(m,open(output_dir.joinpath('data_metadata.json'),'w'))\n",
    "\n",
    "with open(output_dir.joinpath('readme.md'),'w') as fo:\n",
    "    fo. write(\"\"\"\n",
    "Files:\n",
    "- ee_ee_scraping_earth_engine_sentinel_2-austin_leaks- cached tiff files\n",
    "- script_metadata.json - information on scraping script\n",
    "- data.h5 contains X, y, and t.\n",
    "    - X: tiff files for each band loaded into an array of shape (Leak, Bands, width, length)\n",
    "    - y: True for before the leak, False for after\n",
    "    - t: time before leak (can be negative) in seconds\n",
    "- data_metadata: array of metadata for each leak in X. Each contain info on leak, image, and image search\n",
    "    \n",
    "Loading: \n",
    "```py\n",
    "# load\n",
    "metadatas = json.load(open('data_metadata.json'))\n",
    "with h5py.File('data.h5','r') as h5f:\n",
    "    X2 = h5f['X'][:]\n",
    "    y2 = h5f['y'][:]\n",
    "    t2 = h5f['t'][:]\n",
    "y\n",
    "```\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-03-17T09:34:49.594Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test load\n",
    "metadatas = json.load(open(output_dir.joinpath('data_metadata.json')))\n",
    "with h5py.File(output_dir.joinpath('data.h5'),'r') as h5f:\n",
    "    X2 = h5f['X'][:]\n",
    "    y2 = h5f['y'][:]\n",
    "    t2 = h5f['t'][:]\n",
    "X2.shape, y2, t2, metadatas[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-03-17T09:34:50.416Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (with sys packages)",
   "language": "python",
   "name": "py3syspck"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  },
  "toc": {
   "nav_menu": {
    "height": "96px",
    "width": "251px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
