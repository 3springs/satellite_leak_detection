{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook scrapes satellite images for each leak repair. For each location it gets a NxM rectangle around the leak before and after it was repaired. Then it collated all the data into h5 files and all the metadata into json files.\n",
    "\n",
    "It takes days to run because of rate limiting on the google earth api. Because of limited satelite coverage you might find matches for only 10% of the leaks.\n",
    "\n",
    "## Modifying\n",
    "\n",
    "- make sure google earth is setup\n",
    "- load leaks, so they pass the asserts\n",
    "- change params\n",
    "- run rest of cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-18T10:08:09.472506Z",
     "start_time": "2017-03-18T18:08:07.757873+08:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%.4f'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from path import Path\n",
    "import arrow\n",
    "import json\n",
    "import pytz\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import re, os, collections, itertools, uuid, logging\n",
    "import tempfile\n",
    "import tables\n",
    "\n",
    "import zipfile\n",
    "import urllib\n",
    "\n",
    "import ee\n",
    "import pyproj\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (15, 5) # bigger plots\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "%precision 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-18T10:08:09.477069Z",
     "start_time": "2017-03-18T18:08:09.474514+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-18T10:08:12.811993Z",
     "start_time": "2017-03-18T18:08:09.479209+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "helper_dir = str(Path('..').abspath())\n",
    "if helper_dir not in os.sys.path:\n",
    "    os.sys.path.append(helper_dir)\n",
    "    \n",
    "from leak_helpers.earth_engine import display_ee, get_boundary, tifs2np, bands_s2, download_image, bands_s2, bands_s1, bands_l7, bands_l8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-18T10:08:12.848915Z",
     "start_time": "2017-03-18T18:08:12.813510+08:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Path('/tmp/testing_earth_engine-l7-AUTX-48wtrw24-20170318-10-08-12'),\n",
       " Path('../data/20170314-05-26-52_testing_earth_engine-l7-AUTX_v2'),\n",
       " Path('../data/20170314-05-26-52_testing_earth_engine-l7-AUTX_v2/ee_l7_AUTX-leaks_cache_v2'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "crs_grid = 3857\n",
    "notebook_name='testing_earth_engine-l7-AUTX'\n",
    "ts=arrow.utcnow().format('YYYYMMDD-HH-mm-ss')\n",
    "data_dir = Path('../data/')\n",
    "bands = bands_l7\n",
    "\n",
    "# since the lowest res band is 60m and I want to capture neighbours I should get 6+ pixels\n",
    "pixel_length = 25.0\n",
    "resolution_min = 15.0 # m\n",
    "time_bin_delta = 60*60*24*28 # how long before a leak to look (in seconds)\n",
    "# TODO get closest but let me filter for time\n",
    "\n",
    "# init\n",
    "temp_dir = Path(tempfile.mkdtemp(prefix=notebook_name+'-', suffix='-'+ts))\n",
    "# output_dir = data_dir.joinpath('{ts:}_{notebook_name:}'.format(ts=ts,notebook_name=notebook_name))\n",
    "output_dir = Path('../../data/scraped_satellite_images/20170314-05-26-52_testing_earth_engine-l7-AUTX_v2')\n",
    "cache_dir = output_dir.joinpath('ee_l7_AUTX-leaks_cache_v2')\n",
    "\n",
    "output_dir.makedirs_p()\n",
    "temp_dir.makedirs_p()\n",
    "cache_dir.makedirs_p()\n",
    "\n",
    "logger = logging.getLogger(notebook_name)\n",
    "logger.setLevel(logging.WARN)\n",
    "\n",
    "crs_grid_proj = pyproj.Proj('+init=epsg:%s'%crs_grid)\n",
    "\n",
    "temp_dir, output_dir, cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-18T10:08:12.870492Z",
     "start_time": "2017-03-18T18:08:12.850253+08:00"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "metadata_file = output_dir.joinpath('script_metadata.json')\n",
    "\n",
    "# write metadata to json\n",
    "metadata = dict(\n",
    "    pixel_length=pixel_length,\n",
    "    resolution_min=resolution_min,\n",
    "    bands=bands,\n",
    "    ts=ts,\n",
    "    notebook_name=notebook_name,\n",
    "    crs_grid=crs_grid,\n",
    "    cache_dir=str(cache_dir),\n",
    "    temp_dir=str(temp_dir),\n",
    "    output_dir=str(output_dir),\n",
    ")\n",
    "json.dump(metadata, open(metadata_file,'w'))\n",
    "\n",
    "                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# earth engine\n",
    "\n",
    "Setup instructions here\n",
    "- first need to apply for an account and wait ~ 1day\n",
    "- https://developers.google.com/earth-engine/python_install#setting-up-authentication-credentials\n",
    "\n",
    "Refs:\n",
    "- api https://developers.google.com/earth-engine/\n",
    "- code examples https://code.earthengine.google.com/\n",
    "- sentinel1 https://developers.google.com/earth-engine/sentinel1\n",
    "    - `ee.ImageCollection('COPERNICUS/S2_GRD');`\n",
    "    - `ee.ImageCollection('COPERNICUS/S1_GRD');`\n",
    "- keras and google earth https://github.com/patrick-dd/landsat-landstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-18T10:08:15.140158Z",
     "start_time": "2017-03-18T18:08:12.872467+08:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "# test earth-engine setup\n",
    "from oauth2client import crypt # should have not error\n",
    "import ee\n",
    "ee.Initialize() # should give no errors, if so follow instructions\n",
    "\n",
    "\n",
    "# test\n",
    "image = ee.Image('srtm90_v4')\n",
    "assert image.getInfo()=={'type': 'Image', 'properties': {'system:time_start': 950227200000, 'system:asset_size': 18827626666, 'system:time_end': 951177600000}, 'bands': [{'data_type': {'type': 'PixelType', 'max': 32767, 'min': -32768, 'precision': 'int'}, 'crs': 'EPSG:4326', 'id': 'elevation', 'dimensions': [432000, 144000], 'crs_transform': [0.000833333333333, 0.0, -180.0, 0.0, -0.000833333333333, 60.0]}], 'id': 'srtm90_v4', 'version': 1463778555689000}\n",
    "print('ok')\n",
    "\n",
    "# ee.Geometry.Point([117.21079620254062, -30.94712385398404])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load leaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-18T10:08:17.925179Z",
     "start_time": "2017-03-18T18:08:15.141537+08:00"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'leaks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c524ae34d362>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mleaks_ATX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'leak_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mleaks_ATX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOBJECTID\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'ATX-%s'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# leaks=leaks_ATX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mleaks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleaks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleak_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleaks_ATX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'leaks' is not defined"
     ]
    }
   ],
   "source": [
    "# load wa leaks\n",
    "leaks_ATX = gpd.read_file(data_dir.joinpath('austin_leaks/derived/austin_leaks-repairs.geojson'))\n",
    "\n",
    "\n",
    "# they have to be after launch\n",
    "s3_launch_ts=pd.Timestamp('1 Jan 1999')\n",
    "leaks_ATX = leaks_ATX[pd.to_datetime(leaks_ATX.COMPDTTM)>=s3_launch_ts]\n",
    "leaks_ATX['REPO_Date']=leaks_ATX['COMPDTTM']\n",
    "leaks_ATX['leak_id']=leaks_ATX.OBJECTID.apply(lambda x:'ATX-%s'%x)\n",
    "# leaks=leaks_ATX\n",
    "leaks.index = leaks.leak_id\n",
    "len(leaks_ATX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-18T10:08:17.925652Z",
     "start_time": "2017-03-18T10:08:09.292Z"
    }
   },
   "outputs": [],
   "source": [
    "# choose one leak for now\n",
    "leak = leaks_ATX.sample()\n",
    "leak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-15T10:46:31.548298",
     "start_time": "2017-01-15T10:46:31.546367"
    }
   },
   "source": [
    "# Fetching sentinal-1 and sentinel 2 images\n",
    "\n",
    "For a leak repair, grab the image before and after it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note roughly 10% have results for a 1 day temporal bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-18T10:08:28.842346Z",
     "start_time": "2017-03-18T18:08:28.836767+08:00"
    }
   },
   "outputs": [],
   "source": [
    "def get_cached_ids():\n",
    "    cache_dirs = [str(f.relpath(cache_dir)).split('_')[0] for f in cache_dir.listdir()]\n",
    "    return cache_dirs\n",
    "\n",
    "def init_cache(leak_id):\n",
    "    \"\"\"We will cache downloads in folders like 'id_after'\"\"\"\n",
    "    if leak_id:\n",
    "        cache_subdir = cache_dir.joinpath(leak_id+'_after')\n",
    "        cache_subdir.makedirs_p()\n",
    "        cache_subdir = cache_dir.joinpath(leak_id+'_before')\n",
    "        cache_subdir.makedirs_p()\n",
    "    return get_cached_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each point\n",
    "- find the nearest image before the repair\n",
    "- and the soonest image after repair\n",
    "- save a part of each with metadata\n",
    "\n",
    "Later we can filter, interpolate, and read into numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-18T10:08:30.447941Z",
     "start_time": "2017-03-18T18:08:30.445152+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distance = resolution_min*(pixel_length/2.0-0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-18T10:08:17.926512Z",
     "start_time": "2017-03-18T10:08:10.530Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# test with one image\n",
    "for i in [10,50,100,1000,2000]:\n",
    "    leak=leaks_ATX.iloc[[i]]\n",
    "    leak_id = str(leak.OBJECTID.values[0])\n",
    "\n",
    "    repo_date_ts = arrow.get(leak.REPO_Date.values[0]).timestamp\n",
    "    boundary = get_boundary(leak, distance=distance)\n",
    "    sentinel2_before = ee.ImageCollection('LANDSAT/LE7_L1T')\\\n",
    "        .filterBounds(boundary)\\\n",
    "        .filterDate(933828614605,1488776737937)\\\n",
    "        .sort('system:time_start', opt_ascending=False) # first will be latest\n",
    "    image = ee.Image(sentinel2_before.first()).clip(boundary)\n",
    "    image.getInfo()\n",
    "    name=leak_id+'_after'\n",
    "    path,files=download_image(\n",
    "        image, \n",
    "        scale=resolution_min, \n",
    "        crs=crs_grid, \n",
    "        name=name,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "    data = tifs2np(path,files,bands=bands_l7)\n",
    "    print(i, [(d.shape,d.sum()) for d in data])\n",
    "    for d in data:\n",
    "        assert d.shape[0]==pixel_length, 'the downloaded image is the wrong size, tweak distance'\n",
    "        assert d.shape[1]==pixel_length\n",
    "    assert np.sum(data)!=0,'should not be empty (make sure you are using the right bands)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-15T01:59:52.869742Z",
     "start_time": "2017-03-15T09:59:52.865652+08:00"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-15T14:34:52.122574Z",
     "start_time": "2017-03-15T22:34:52.120409+08:00"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-19T08:08:16.798286Z",
     "start_time": "2017-03-18T18:08:31.789754+08:00"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecbee146c27a476ca62d495430ac5cbe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17868 Earth Engine memory capacity exceeded.\n",
      "17869 Earth Engine memory capacity exceeded.\n",
      "17940 Earth Engine memory capacity exceeded.\n",
      "17941 Earth Engine memory capacity exceeded.\n",
      "17945 Earth Engine memory capacity exceeded.\n",
      "17956 Earth Engine memory capacity exceeded.\n",
      "17958 Earth Engine memory capacity exceeded.\n",
      "17967 Earth Engine memory capacity exceeded.\n",
      "17971 Earth Engine memory capacity exceeded.\n",
      "17999 Earth Engine memory capacity exceeded.\n",
      "18012 Earth Engine memory capacity exceeded.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "cached_ids = get_cached_ids()\n",
    "\n",
    "def get_image_for_leak(i, cached_ids=cached_ids):    \n",
    "    leak = leaks_ATX.iloc[[i]]\n",
    "    repo_date_ts = arrow.get(leak.REPO_Date.values[0]).timestamp\n",
    "#     distance = resolution_min*(pixel_length/2.0+1)\n",
    "    \n",
    "    \n",
    "    # crappy way or recording that we tried this one\n",
    "    leak_id = str(leak.OBJECTID.values[0])\n",
    "    if leak_id in cached_ids:\n",
    "        logger.info('Skipping cached download for leak id %s ',leak_id)\n",
    "        return\n",
    "    \n",
    "    boundary = get_boundary(leak, distance=distance)\n",
    "    \n",
    "    # get image day before    \n",
    "    sentinel2_before = ee.ImageCollection('LANDSAT/LE7_L1T')\\\n",
    "        .filterBounds(boundary)\\\n",
    "        .filterDate((repo_date_ts-time_bin_delta)*1000,(repo_date_ts)*1000)\\\n",
    "        .sort('system:time_start', opt_ascending=False) # first will be latest\n",
    "    \n",
    "    results = sentinel2_before.size().getInfo()\n",
    "    if results<1:\n",
    "        logger.info('Error no results for day before %s',leak_id)\n",
    "        cached_ids = init_cache(leak_id) # so we know there where no results\n",
    "        return\n",
    "        \n",
    "    # get image day after\n",
    "    sentinel2_after = ee.ImageCollection('LANDSAT/LE7_L1T')\\\n",
    "        .filterBounds(boundary)\\\n",
    "        .filterDate((repo_date_ts)*1000,(repo_date_ts+time_bin_delta*6)*1000)\\\n",
    "        .sort('system:time_start', opt_ascending=True) # first will be earliest\n",
    "        \n",
    "    results = sentinel2_after.size().getInfo()\n",
    "    if results<1:\n",
    "        logger.info('Error no results for day after, id %s',leak_id)\n",
    "        cached_ids = init_cache(leak_id) # so we know there where no results\n",
    "        return\n",
    "        \n",
    "    # download as save images    \n",
    "    logger.info('results for %s', leak_id)\n",
    "    image = ee.Image(sentinel2_before.first()).clip(boundary)\n",
    "    name=leak_id+'_before'\n",
    "    path,files=download_image(\n",
    "        image, \n",
    "        scale=resolution_min, \n",
    "        crs=crs_grid, \n",
    "        name=name,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "    # also save metadata so we can filter by date\n",
    "    with open(path.joinpath('metadata.json'), 'w') as fo:\n",
    "        metadata = dict(\n",
    "            image=image.getInfo(),\n",
    "            scale=resolution_min,\n",
    "            crs=crs_grid,\n",
    "            name=name,\n",
    "            distance=distance,\n",
    "            leak=json.loads(leak.to_json())\n",
    "        )\n",
    "        json.dump(metadata, fo)\n",
    "\n",
    "    image = ee.Image(sentinel2_after.first()).clip(boundary)\n",
    "    name=leak_id+'_after'\n",
    "    path,files=download_image(\n",
    "        image, \n",
    "        scale=resolution_min, \n",
    "        crs=crs_grid, \n",
    "        name=name,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "    with open(path.joinpath('metadata.json'), 'w') as fo:\n",
    "        metadata = dict(\n",
    "            image=image.getInfo(),\n",
    "            scale=resolution_min,\n",
    "            crs=crs_grid,\n",
    "            name=name,\n",
    "            distance=distance,\n",
    "            leak=json.loads(leak.to_json())\n",
    "        )\n",
    "        json.dump(metadata, fo)\n",
    "        \n",
    "for i in tqdm(range(len(leaks_ATX))):\n",
    "    try:\n",
    "        get_image_for_leak(i)\n",
    "    except urllib.error.HTTPError as e:\n",
    "        print(i,e)\n",
    "        if e.code == 429:\n",
    "             time.sleep(13);\n",
    "    except Exception as e:\n",
    "        print(i,e)\n",
    "        ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-18T04:36:19.085524Z",
     "start_time": "2017-03-18T12:36:19.082804+08:00"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parsing tiffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-15T04:39:05.795321Z",
     "start_time": "2017-03-15T12:39:05.788009+08:00"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-20T06:54:54.295929Z",
     "start_time": "2017-03-20T13:49:03.076436+08:00"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b66a8a0c73ef43569f24d1934b59e24a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid metadata.json, deleted folder ../data/20170314-05-26-52_testing_earth_engine-l7-AUTX_v2/ee_l7_AUTX-leaks_cache_v2/65498_before_3857_15.0, please rerun scraping cell to rescrape this image\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(40745, 0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This loads it as X and y for machine learning, and also time and metadata so we can filter\n",
    "import shapely\n",
    "X = []\n",
    "y = []\n",
    "t = []\n",
    "m = []\n",
    "discarded=[]\n",
    "cdirs = [cdir for cdir in cache_dir.listdir() if ('_after_' in cdir) or ('_before_' in cdir)]\n",
    "for path in tqdm(cdirs):\n",
    "    if not path.isdir(): continue\n",
    "    files = [file.relpath(path) for file in path.listdir() if file.isfile() and file.endswith('.tif')]\n",
    "    if files:\n",
    "        # check metadata\n",
    "        try:\n",
    "            metadata = json.load(open(path.joinpath('metadata.json')))\n",
    "        except (FileNotFoundError, ValueError) as e:\n",
    "            path.move(path.dirname().dirname().joinpath('.deleteme-'+str(uuid.uuid4())))\n",
    "            if '_after_' in path: # also delete the before path                \n",
    "                path_after = Path(path.replace('_after_','_before_'))\n",
    "                if path_after.isdir():\n",
    "                    path_after.move(path.dirname().dirname().joinpath('.deleteme-'+str(uuid.uuid4())))\n",
    "            logger.error('Invalid metadata.json, deleted folder %s, please rerun scraping cell to rescrape this image', path)\n",
    "            continue\n",
    "        \n",
    "        # e.g. lets filter it so \"before\" image are only 1 day before\n",
    "        if '_before_' in path.basename():\n",
    "            yy = True\n",
    "        else:\n",
    "            yy = False\n",
    "        \n",
    "        # work out time gap too\n",
    "        t1 = arrow.get(metadata['image']['properties']['system:time_end']/1000)\n",
    "        t0 = arrow.get(metadata['leak']['features'][0]['properties']['REPO_Date'])\n",
    "        td=t1-t0\n",
    "        tt = td.total_seconds()\n",
    "        \n",
    "        # load data\n",
    "        data = tifs2np(path,files,bands=bands)\n",
    "             \n",
    "        # check we don't have empty bands 1-13\n",
    "        empty_bands = np.array([d.sum() for d in data])==0\n",
    "        \n",
    "        # lets check we didn't get the edge of an image\n",
    "        bbox = np.array(metadata['image']['properties']['system:footprint']['coordinates'][0])\n",
    "        loc = metadata['leak']['features'][0]['geometry']['coordinates']\n",
    "        minx=bbox[:,0].min()\n",
    "        maxx=bbox[:,0].max()\n",
    "        miny=bbox[:,1].min()\n",
    "        maxy=bbox[:,1].max()\n",
    "        bbox_shp = shapely.geometry.box(\n",
    "            minx=minx,\n",
    "            maxx=maxx,\n",
    "            miny=miny,\n",
    "            maxy=maxy\n",
    "        )\n",
    "        loc_shp = shapely.geometry.Point(loc[0],loc[1])\n",
    "        shapely.geometry.GeometryCollection([bbox_shp, loc_shp])\n",
    "        try:\n",
    "            assert loc_shp.intersects(bbox_shp), 'leak location should be inside image'\n",
    "            assert bbox_shp.centroid.almost_equals(loc_shp, decimal=5), 'leak should be near center of image'\n",
    "            assert (np.array([d.shape for d in data])==pixel_length).all(), 'image area should be the right amount of pixels'\n",
    "            assert (maxx-minx)/(maxy-miny)<1.3, 'should be roughly square'\n",
    "            assert (maxx-minx)/(maxy-miny)>0.7, 'should be roughly square'\n",
    "            assert not empty_bands.all(), 'should not have all bands empty'\n",
    "        except Exception as exc:\n",
    "            print(path, exc)\n",
    "#             raise(exc)\n",
    "            discarded.append(path)\n",
    "        else:\n",
    "            X.append(data)\n",
    "            y.append(yy)\n",
    "            t.append(tt)\n",
    "            m.append(metadata)\n",
    "\n",
    "len(X), len(discarded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-20T05:48:19.175469Z",
     "start_time": "2017-03-20T13:48:07.994632+08:00"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-20T07:48:45.188727Z",
     "start_time": "2017-03-20T15:48:45.173290+08:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('../data/20170314-05-26-52_testing_earth_engine-l7-AUTX_v2/ee_l7_AUTX-leaks_cache_v2/70616_before_3857_15.0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-20T07:48:45.456224Z",
     "start_time": "2017-03-20T15:48:45.389047+08:00"
    }
   },
   "outputs": [],
   "source": [
    "# shuffle\n",
    "from sklearn.utils import shuffle\n",
    "X,y,m= shuffle(X,y,m,random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-03-20T07:48:45.667Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount of each band [('B1', 0), ('B2', 0), ('B3', 0), ('B4', 0), ('B5', 0), ('B6_VCID_1', 0), ('B6_VCID_2', 0), ('B7', 0), ('B8', 0)]\n",
      "mean amount of bands 0.0\n"
     ]
    }
   ],
   "source": [
    "# which bands do we have?\n",
    "a=np.array([x.sum(-1).sum(-1)==0 for x in X])\n",
    "print('amount of each band',list(zip(bands,a.sum(0))))\n",
    "print('mean amount of bands',a.sum(1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-03-20T07:48:46.082Z"
    }
   },
   "outputs": [],
   "source": [
    "# save using hdf5 (so keras can easily load it) and json \n",
    "import h5py\n",
    "h5file = output_dir.joinpath('data.h5')\n",
    "with h5py.File(h5file, 'w') as h5f:\n",
    "    h5f.create_dataset('X', data=X)\n",
    "    h5f.create_dataset('y', data=y)\n",
    "\n",
    "json.dump(m,open(output_dir.joinpath('data_metadata.json'),'w'))\n",
    "\n",
    "with open(output_dir.joinpath('readme.md'),'w') as fo:\n",
    "    fo. write(\"\"\"\n",
    "Files:\n",
    "- ee_S1_AUTX-leaks_cache- cached tiff files\n",
    "- script_metadata.json - information on scraping script\n",
    "- data.h5 contains X, y, and t.\n",
    "    - X: tiff files for each band loaded into an array of shape (Leak, Bands, width, length)\n",
    "    - y: True for before the leak, False for after\n",
    "- data_metadata: array of metadata for each leak in X. Each contain info on leak, image, and image search\n",
    "    \n",
    "Loading: \n",
    "```py\n",
    "# load\n",
    "metadatas = json.load(open('data_metadata.json'))\n",
    "with h5py.File('data.h5','r') as h5f:\n",
    "    X2 = h5f['X'][:]\n",
    "    y2 = h5f['y'][:]\n",
    "y\n",
    "```\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-03-20T07:48:46.458Z"
    }
   },
   "outputs": [],
   "source": [
    "# test load\n",
    "metadatas = json.load(open(output_dir.joinpath('data_metadata.json')))\n",
    "with h5py.File(output_dir.joinpath('data.h5'),'r') as h5f:\n",
    "    X2 = h5f['X'][:]\n",
    "    y2 = h5f['y'][:]\n",
    "X2.shape, y2, metadatas[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (with sys packages)",
   "language": "python",
   "name": "py3syspck"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  },
  "toc": {
   "nav_menu": {
    "height": "96px",
    "width": "251px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
